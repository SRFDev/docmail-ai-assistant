{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94cdef11-0148-46be-9087-9040f551ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.2\n",
      "    Uninstalling pip-25.2:\n",
      "      Successfully uninstalled pip-25.2\n",
      "Successfully installed pip-25.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404dc0b7-e377-43a5-a24c-737448252ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base PyTorch: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Base PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "862d6313-a1e5-4092-a6f9-b6b93678aed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-xksp_2k8/unsloth_ee7f7dbc56d14d38baebd4e354d77871\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-xksp_2k8/unsloth_ee7f7dbc56d14d38baebd4e354d77871\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 8ea5338154859ed25b50366cb1264ed4d933eae3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (2.8.0+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (3.0.3)\n",
      "Collecting xformers@ https://download.pytorch.org/whl/cu129/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading https://download.pytorch.org/whl/cu129/xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.6/122.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.12.7 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading unsloth_zoo-2025.12.7-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (0.23.0+cu128)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
      "Collecting tyro (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-1.0.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.57.3,>=4.51.3 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting tqdm (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (7.1.0)\n",
      "Collecting wheel>=0.42.0 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting accelerate>=0.34.1 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting protobuf (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting huggingface_hub>=0.34.0 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hf_transfer (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (11.0.0)\n",
      "Collecting regex (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (0.28.1)\n",
      "Collecting xxhash (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub>=0.34.0->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface_hub>=0.34.0->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface_hub>=0.34.0->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading typer_slim-0.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting huggingface_hub>=0.34.0 (from unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.57.3,>=4.51.3->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.57.3,>=4.51.3->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface_hub>=0.34.0->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth_zoo>=2025.12.7->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting diffusers (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading diffusers-0.36.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting importlib_metadata (from diffusers->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->diffusers->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu128-torch280]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.12.7-py3-none-any.whl (290 kB)\n",
      "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading tyro-1.0.3-py3-none-any.whl (180 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading diffusers-0.36.0-py3-none-any.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2025.12.9-py3-none-any.whl size=382658 sha256=6bcac8769f3facd387f57ec133ffe217ed9de92110c27308363b203fe7c58659\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8j7bnav7/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
      "Successfully built unsloth\n",
      "Installing collected packages: torchao, pytz, zipp, xxhash, wheel, unsloth, tzdata, typeguard, tqdm, sentencepiece, safetensors, regex, pyarrow, protobuf, propcache, multidict, msgspec, hf-xet, hf_transfer, frozenlist, docstring-parser, dill, aiohappyeyeballs, yarl, tyro, pandas, multiprocess, importlib_metadata, huggingface_hub, aiosignal, tokenizers, diffusers, aiohttp, xformers, transformers, cut_cross_entropy, bitsandbytes, accelerate, peft, datasets, trl, unsloth_zoo\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/42\u001b[0m [unsloth_zoo]loth_zoo]]sets]e]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 bitsandbytes-0.49.0 cut_cross_entropy-25.1.1 datasets-4.3.0 diffusers-0.36.0 dill-0.4.0 docstring-parser-0.17.0 frozenlist-1.8.0 hf-xet-1.2.0 hf_transfer-0.1.9 huggingface_hub-0.36.0 importlib_metadata-8.7.1 msgspec-0.20.0 multidict-6.7.0 multiprocess-0.70.16 pandas-2.3.3 peft-0.18.0 propcache-0.4.1 protobuf-6.33.2 pyarrow-22.0.0 pytz-2025.2 regex-2025.11.3 safetensors-0.7.0 sentencepiece-0.2.1 tokenizers-0.22.1 torchao-0.15.0 tqdm-4.67.1 transformers-4.57.3 trl-0.24.0 typeguard-4.4.4 tyro-1.0.3 tzdata-2025.3 unsloth-2025.12.9 unsloth_zoo-2025.12.7 wheel-0.45.1 xformers-0.0.32.post2 xxhash-3.6.0 yarl-1.22.0 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"unsloth[cu128-torch280] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a984d8-decc-4291-9955-1d38d34a896a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xformers in /usr/local/lib/python3.12/dist-packages (0.0.32.post2)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.1.2)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from xformers) (2.8.0+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.4.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.3.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->xformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->xformers) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c65aff-9a83-4c0f-9ec3-e93675c58107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "⬇️  Downloading Llama 3 8B...\n",
      "==((====))==  Unsloth 2025.12.9: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5932ed2c3f34cd38d7675a8d62d154c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d43bce77a9447c8348b07cfd95c689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/198 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5813e84910df4f0f97fdc96d0985cc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36062b7a9edf4080802f3d024dac1a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44927b5304704042ac917fc53e6c0805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Loaded. Attaching LoRA Adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Ready for Data.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None # Auto detection\n",
    "load_in_4bit = True \n",
    "\n",
    "print(\"⬇️  Downloading Llama 3 8B...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\", \n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"✅ Model Loaded. Attaching LoRA Adapters...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Rank\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")\n",
    "print(\"🚀 Ready for Data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697d6265-5af8-4e27-bf86-9f17b912ca78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a594bcf8aa4c7288b36decc3e263ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bd755098924a099af9c1ef9f23408b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Formatted. Training on 491 records.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Define the \"Alpaca\" Prompt Template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Critical: Tells the model where to stop generating.\n",
    "\n",
    "# 2. Formatting Function\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Fill the template + Append EOS token\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# 3. Load Data\n",
    "# Ensure the filename matches exactly what you uploaded\n",
    "dataset = load_dataset(\"json\", data_files=\"physician_style_dataset.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "print(f\"✅ Data Formatted. Training on {len(dataset)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b28422-e8e4-4571-b963-fa5bf23e364b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b7a82e694c4aa88ba9b39a5ec4a85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=260):   0%|          | 0/491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Initiating Training Sequence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 491 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.868400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.842300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.813400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.372400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.970900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.960600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.974400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.977300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.025600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training Complete.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60, # 1 Epoch (approx)\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"🔥 Initiating Training Sequence...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(\"✅ Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca5502ac-3b98-4922-b4ca-31f370dd1848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Reply...\n",
      "\n",
      "==================== MODEL OUTPUT ====================\n",
      "Dear Mr. Jones,\n",
      "\n",
      "Thank you for reaching out about your missed Lisinopril dose. I understand that missing a medication can be concerning, especially when it's for heart health.\n",
      "\n",
      "For missed doses of Lisinopril, the general recommendation is to take the missed dose as soon as possible if it's within 6-8 hours of your usual time. If it's been more than 8 hours, just skip that dose and continue with your regular schedule. Don't double up on doses or take extra pills.\n",
      "\n",
      "If you're experiencing any symptoms like chest pain, shortness of breath, dizziness, or your blood pressure is unusually high or low, please contact us immediately or go to the emergency room.\n",
      "\n",
      "If you're concerned about the impact of this missed dose, please call our office at [phone number] to discuss your specific situation.\n",
      "\n",
      "You can also find more information about missed doses in our patient portal under \"Medication Management.\"\n",
      "\n",
      "We're here to help, and please don't hesitate to contact us if you have any questions or concerns.\n",
      "\n",
      "Sincerely,\n",
      "Henry Smith, Physician's Assistant\n",
      "Hempstead Cardiology Associates\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Define the test case\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"You are a physician's assistant drafting a reply to a patient's email. Be professional, empathetic, and cautious. Your name is Henry Smith and your practice name is Hempstead Cardiology Associates\", # Instruction\n",
    "        \"Hi Dr. Smith, I forgot to take my heart medication (Lisinopril) yesterday. I'm worried. Should I take two pills today to make up for it? - Jeremiah Jones\", # Input\n",
    "        \"\", # Output - leave blank for generation\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "print(\"Generating Reply...\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
    "result = tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Clean up the output to show just the response\n",
    "response_text = result[0].split(\"### Response:\\n\")[-1].replace(EOS_TOKEN, \"\")\n",
    "print(\"\\n\" + \"=\"*20 + \" MODEL OUTPUT \" + \"=\"*20)\n",
    "print(response_text)\n",
    "print(\"=\"*54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55202a89-c41e-4d75-8108-5a08df55ad02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Converting and Saving GGUF...\n",
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /workspace/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|██████████| 4/4 [00:00<00:00, 19043.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|██████████| 4/4 [00:31<00:00,  7.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/docmail_llama3_model`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: llama.cpp folder exists but binaries not found - will rebuild\n",
      "Unsloth: Updating system package directories\n",
      "Unsloth: All required system packages already installed!\n",
      "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
      "Unsloth: Install GGUF and other packages\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: GGUF conversion failed: [FAIL] Command `uv pip install gguf protobuf sentencepiece mistral_common` failed with exit code 2\nstdout: \u001b[1m\u001b[31merror\u001b[39m\u001b[0m: No virtual environment found; run `\u001b[32muv venv\u001b[39m` to create an environment, or pass `\u001b[32m--system\u001b[39m` to install into a non-virtual environment\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth/save.py:1192\u001b[39m, in \u001b[36msave_to_gguf\u001b[39m\u001b[34m(model_name, model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, is_vlm, is_gpt_oss)\u001b[39m\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     quantizer_location, converter_location = \u001b[43mcheck_llama_cpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1193\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth: llama.cpp found in the system. Skipping installation.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/llama_cpp.py:345\u001b[39m, in \u001b[36mcheck_llama_cpp\u001b[39m\u001b[34m(llama_cpp_folder)\u001b[39m\n\u001b[32m    344\u001b[39m     files_found = glob.glob(os.path.join(llama_cpp_folder, \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    346\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: No working quantizer found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    347\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFiles in directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(os.path.basename(f)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mf\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mfiles_found[:\u001b[32m20\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    348\u001b[39m     )\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsloth: No working quantizer found in llama.cpp\nFiles in directory: AGENTS.md, AUTHORS, CLAUDE.md, CMakeLists.txt, CMakePresets.json, CODEOWNERS, CONTRIBUTING.md, LICENSE, Makefile, README.md, SECURITY.md, benches, build-xcframework.sh, ci, cmake, common, convert_hf_to_gguf.py, convert_hf_to_gguf_update.py, convert_llama_ggml_to_gguf.py, convert_lora_to_gguf.py",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth/save.py:1967\u001b[39m, in \u001b[36munsloth_save_pretrained_gguf\u001b[39m\u001b[34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   1966\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m     all_file_locations, want_full_precision, is_vlm_update = \u001b[43msave_to_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_sentencepiece\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_conversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass VLM flag\u001b[39;49;00m\n\u001b[32m   1976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_gpt_oss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_gpt_oss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass gpt_oss Flag\u001b[39;49;00m\n\u001b[32m   1977\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1978\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth/save.py:1202\u001b[39m, in \u001b[36msave_to_gguf\u001b[39m\u001b[34m(model_name, model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, is_vlm, is_gpt_oss)\u001b[39m\n\u001b[32m   1201\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1202\u001b[39m         quantizer_location, converter_location = \u001b[43minstall_llama_cpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgpu_support\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GGUF conversion doesn't need CUDA\u001b[39;49;00m\n\u001b[32m   1204\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprint_output\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[38;5;66;03m# Step 2: Download and patch converter script\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/llama_cpp.py:417\u001b[39m, in \u001b[36minstall_llama_cpp\u001b[39m\u001b[34m(llama_cpp_folder, llama_cpp_targets, print_output, gpu_support, just_clone_repo)\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Install GGUF and other packages\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m \u001b[43mtry_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpip\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m install gguf protobuf sentencepiece mistral_common\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m just_clone_repo: \u001b[38;5;28;01mreturn\u001b[39;00m llama_cpp_folder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/llama_cpp.py:277\u001b[39m, in \u001b[36mtry_execute\u001b[39m\u001b[34m(command, sudo, print_output, print_outputs, cwd, system_type)\u001b[39m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stderr: error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstderr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# Process output\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: [FAIL] Command `uv pip install gguf protobuf sentencepiece mistral_common` failed with exit code 2\nstdout: \u001b[1m\u001b[31merror\u001b[39m\u001b[0m: No virtual environment found; run `\u001b[32muv venv\u001b[39m` to create an environment, or pass `\u001b[32m--system\u001b[39m` to install into a non-virtual environment\n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save to GGUF format (Quantized for Mac/Local use)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# We use q4_k_m (4-bit medium) which is the industry standard balance of speed/quality.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m💾 Converting and Saving GGUF...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocmail_llama3_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mq4_k_m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Conversion Complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/unsloth/save.py:1987\u001b[39m, in \u001b[36munsloth_save_pretrained_gguf\u001b[39m\u001b[34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   1980\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1981\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: GGUF conversion failed in Kaggle environment.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1982\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis is likely due to the 20GB disk space limit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1983\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTry saving to /tmp directory or use a smaller model.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1984\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1985\u001b[39m         )\n\u001b[32m   1986\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1987\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: GGUF conversion failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# Step 9: Create Ollama modelfile\u001b[39;00m\n\u001b[32m   1990\u001b[39m modelfile_location = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsloth: GGUF conversion failed: [FAIL] Command `uv pip install gguf protobuf sentencepiece mistral_common` failed with exit code 2\nstdout: \u001b[1m\u001b[31merror\u001b[39m\u001b[0m: No virtual environment found; run `\u001b[32muv venv\u001b[39m` to create an environment, or pass `\u001b[32m--system\u001b[39m` to install into a non-virtual environment\n\n"
     ]
    }
   ],
   "source": [
    "# Save to GGUF format (Quantized for Mac/Local use)\n",
    "# We use q4_k_m (4-bit medium) which is the industry standard balance of speed/quality.\n",
    "\n",
    "print(\"💾 Converting and Saving GGUF...\")\n",
    "model.save_pretrained_gguf(\"docmail_llama3_model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "print(\"✅ Conversion Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b87c154-6baf-4a75-af83-a69306992628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gguf in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (6.33.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
      "Requirement already satisfied: mistral_common in /usr/local/lib/python3.12/dist-packages (1.8.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from gguf) (2.1.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from gguf) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from gguf) (4.67.1)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.12.5)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (4.25.1)\n",
      "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (4.15.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (0.12.0)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.32.5)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common) (2.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.4.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.27.1)\n",
      "Requirement already satisfied: pycountry>=23 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common) (24.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (2025.10.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral_common) (2025.11.3)\n",
      "/workspace/llama.cpp\n",
      "Makefile:6: *** Build system changed:\n",
      " The Makefile build has been replaced by CMake.\n",
      "\n",
      " For build instructions see:\n",
      " https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n",
      "\n",
      ".  Stop.\n",
      "Makefile:6: *** Build system changed:\n",
      " The Makefile build has been replaced by CMake.\n",
      "\n",
      " For build instructions see:\n",
      " https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n",
      "\n",
      ".  Stop.\n",
      "/workspace\n",
      "❌ Compilation Failed.\n"
     ]
    }
   ],
   "source": [
    "# 1. Manually install the Python dependencies that 'uv' choked on\n",
    "!pip install gguf protobuf sentencepiece mistral_common\n",
    "\n",
    "# 2. Force compile llama.cpp (Ensure the 'quantize' binary exists)\n",
    "# This fixes the \"No working quantizer found\" error at the top of your log.\n",
    "%cd llama.cpp\n",
    "!make clean\n",
    "!make all -j\n",
    "%cd .. \n",
    "\n",
    "# 3. Verify the binary exists\n",
    "import os\n",
    "if os.path.exists(\"llama.cpp/llama-quantize\"):\n",
    "    print(\"✅ Compilation Successful. Quantizer binary found.\")\n",
    "else:\n",
    "    print(\"❌ Compilation Failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b18511a6-4955-4027-90b4-8f8b98ed7171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmake\n",
      "  Downloading cmake-4.2.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: gguf in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (6.33.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
      "Requirement already satisfied: mistral_common in /usr/local/lib/python3.12/dist-packages (1.8.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from gguf) (2.1.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from gguf) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from gguf) (4.67.1)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.12.5)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (4.25.1)\n",
      "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (4.15.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (0.12.0)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.32.5)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common) (2.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.4.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.27.1)\n",
      "Requirement already satisfied: pycountry>=23 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common) (24.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (2025.10.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral_common) (2025.11.3)\n",
      "Downloading cmake-4.2.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (28.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.9/28.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:00:27\u001b[0mm0:00:01\u001b[0m0:00:03\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cmake\n",
      "Successfully installed cmake-4.2.1\n",
      "/workspace/llama.cpp\n",
      "-- The C compiler identification is GNU 13.3.0\n",
      "-- The CXX compiler identification is GNU 13.3.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.43.0\")\n",
      "-- The ASM compiler identification is GNU\n",
      "-- Found assembler: /usr/bin/cc\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- ggml version: 0.9.4\n",
      "-- ggml commit:  c9a3b40d6\n",
      "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"8.5.0\")\n",
      "-- Configuring done (1.8s)\n",
      "-- Generating done (0.3s)\n",
      "-- Build files have been written to: /workspace/llama.cpp/build\n",
      "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  4%] Built target build_info\n",
      "[  4%] \u001b[1m\u001b[32mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  4%] Built target ggml-base\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[1m\u001b[32mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[ 13%] Built target ggml-cpu\n",
      "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[1m\u001b[32mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "[ 15%] Built target ggml\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[1m\u001b[32mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "[ 88%] Built target llama\n",
      "[ 88%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[100%] \u001b[1m\u001b[32mLinking CXX static library libcommon.a\u001b[0m\n",
      "[100%] Built target common\n",
      "[100%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[100%] \u001b[1m\u001b[32mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "[100%] Built target llama-quantize\n",
      "/workspace\n",
      "✅ CMake Build Successful. Binary located.\n"
     ]
    }
   ],
   "source": [
    "# 1. Ensure build tools are present\n",
    "!pip install cmake gguf protobuf sentencepiece mistral_common\n",
    "\n",
    "# 2. Compile llama.cpp using CMake\n",
    "%cd llama.cpp\n",
    "!rm -rf build\n",
    "!mkdir build\n",
    "# We build Release mode. We skip CUDA for quantization to avoid nvcc version headaches (CPU is fast enough for conversion).\n",
    "!cd build && cmake .. -DGGML_CUDA=OFF && cmake --build . --config Release -j --target llama-quantize\n",
    "\n",
    "# 3. Deployment: Move the binary to the root folder where Unsloth looks\n",
    "# CMake puts binaries in build/bin/. We copy it up.\n",
    "!cp build/bin/llama-quantize .\n",
    "\n",
    "# 4. Compatibility Shim: Unsloth might look for the old name 'quantize'\n",
    "!cp llama-quantize quantize\n",
    "\n",
    "%cd ..\n",
    "\n",
    "# 5. Verification\n",
    "import os\n",
    "if os.path.exists(\"llama.cpp/llama-quantize\"):\n",
    "    print(\"✅ CMake Build Successful. Binary located.\")\n",
    "else:\n",
    "    print(\"❌ Build failed. Check logs above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25b2f295-b08c-4082-be01-549844e3ae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Converting and Saving GGUF...\n",
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /workspace/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|██████████| 4/4 [00:00<00:00, 10242.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|██████████| 4/4 [00:30<00:00,  7.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/docmail_llama3_model`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into bf16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['llama-3-8b.BF16.gguf']\n",
      "Unsloth: [2] Converting GGUF bf16 into q4_k_m. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['llama-3-8b.Q4_K_M.gguf']\n",
      "Unsloth: No Ollama template mapping found for model 'unsloth/llama-3-8b'. Skipping Ollama Modelfile\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model llama-3-8b.Q4_K_M.gguf -p \"why is the sky blue?\"\n",
      "✅ Conversion Complete.\n"
     ]
    }
   ],
   "source": [
    "# Save to GGUF format (Quantized for Mac/Local use)\n",
    "# We use q4_k_m (4-bit medium) which is the industry standard balance of speed/quality.\n",
    "\n",
    "print(\"💾 Converting and Saving GGUF...\")\n",
    "model.save_pretrained_gguf(\"docmail_llama3_model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "print(\"✅ Conversion Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d76f0-cfb1-4579-bed6-8d7160a7502c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
